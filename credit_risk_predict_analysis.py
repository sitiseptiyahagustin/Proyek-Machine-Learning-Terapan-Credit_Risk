# -*- coding: utf-8 -*-
"""credit_risk_predict analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11pAYO3LZ7TeBrh0yvExaLmI_GhUZWHfd

# Proyek Pertama, Predict Analysis: [Credit Risk]
- **Nama:** [Siti Septiyah Agustin]
- **Email:** [sitiagustin561@gmail.com]
- **ID Dicoding:** [sitiseptiyah]

## Import Semua Packages/Library yang Digunakan
"""

!pip install gdown

import pandas as pd
import gdown
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor

"""## Load Dataset"""

!gdown 1RYzcXn6U_mNyuhYn2JO9u51FinoXdP4V

df = pd.read_csv('credit_risk_dataset.csv')
df.head()

"""## EDA"""

df.info()

df.describe()

missing_data = df.isnull().sum()
print("\nJumlah Data yang Hilang per Kolom:")
print(missing_data)

for col in df.columns:
    if df[col].dtype == 'object':
        df[col] = df[col].fillna(df[col].mode()[0])
    else:
        df[col] = df[col].fillna(df[col].mean())

missing_data_after = df.isnull().sum()
print("\nJumlah Data yang Hilang Setelah Pengisian:")
print(missing_data_after)

numerical_columns = df.select_dtypes(include=['number']).columns
categorical_columns = df.select_dtypes(include=['object', 'category']).columns

print("\nKolom Numerikal:", list(numerical_columns))
print("Kolom Kategorikal:", list(categorical_columns))

# Membuat histogram untuk kolom numerik
df[numerical_columns].hist(figsize=(12, 8), bins=20, edgecolor='black', layout=(3, 3))  # 3x3 untuk 9 kolom
plt.suptitle("Distribusi Variabel Numerik", fontsize=14)
plt.show()

# Membuat boxplot untuk kolom numerik
num_rows = (len(numerical_columns) // 3) + 1
df[numerical_columns].plot(kind='box', subplots=True, layout=(num_rows, 3), figsize=(12, 8), patch_artist=True)
plt.suptitle("Boxplot Variabel Numerik", fontsize=14)
plt.show()

sns.pairplot(df)

# Membuat heatmap korelasi antar fitur numerik
plt.figure(figsize=(10, 6))
sns.heatmap(df[numerical_columns].corr(), annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Heatmap Korelasi")
plt.show()

"""## Preprocessing"""

# Menghapus data duplikat
df = df.drop_duplicates()

# Memverifikasi jumlah baris setelah penghapusan duplikat
print(f"Jumlah baris setelah penghapusan duplikat: {df.shape[0]}")

import pandas as pd

# Fungsi untuk mendeteksi dan mengganti outlier dengan median
def replace_outliers_with_median(df, columns, target_column):
    # Menghindari kolom target agar tidak diproses
    columns = [col for col in columns if col != target_column]

    # Iterasi untuk setiap kolom numerik yang tidak termasuk kolom target
    for col in columns:
        # Menghitung Q1 (kuartil pertama) dan Q3 (kuartil ketiga)
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)

        # Menghitung Interquartile Range (IQR)
        IQR = Q3 - Q1

        # Menentukan batas bawah dan atas untuk mendeteksi outlier
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Mengganti outlier dengan nilai median dari kolom tersebut
        median_value = df[col].median()

        # Ganti nilai yang lebih kecil dari lower_bound atau lebih besar dari upper_bound dengan median
        df[col] = df[col].apply(lambda x: median_value if x < lower_bound or x > upper_bound else x)

    return df

# Menentukan kolom numerik untuk mendeteksi outlier
numerical_columns = df.select_dtypes(include=['number']).columns

# Kolom target adalah 'loan_status', yang tidak akan diproses untuk outlier
target_column = 'loan_status'  # Kolom target adalah 'loan_status'

# Mengganti outlier dengan median untuk kolom numerik selain kolom target
df = replace_outliers_with_median(df, numerical_columns, target_column)

# Memverifikasi jumlah baris setelah mengganti outlier
print(f"Jumlah baris setelah mengganti outlier dengan median: {df.shape[0]}")

from sklearn.preprocessing import LabelEncoder
# Melakukan Label Encoding pada kolom kategorikal
categorical_columns = df.select_dtypes(include=['object']).columns
le = LabelEncoder()

for col in categorical_columns:
    df[col] = le.fit_transform(df[col])  # Mengonversi string menjadi angka

# Menampilkan jumlah baris setelah encoding
print(f"Jumlah baris setelah mengganti outlier dan melakukan encoding: {df.shape[0]}")

# Melakukan standarisasi pada fitur
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df.drop('loan_status', axis=1))  # Menstandarkan fitur tanpa target

# Menyimpan kolom target
y = df['loan_status']

# Menampilkan jumlah baris setelah encoding dan scaling
print(f"Jumlah baris setelah mengganti outlier, melakukan encoding, dan scaling: {X_scaled.shape[0]}")

from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler

# Mengisi nilai yang hilang dengan median
imputer = SimpleImputer(strategy='median')

# Mengimputasi (mengisi) nilai NaN pada data fitur
X_imputed = imputer.fit_transform(X_scaled)

# Menggunakan SMOTE untuk menangani ketidakseimbangan kelas
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_imputed, y)

# Sebelum SMOTE
print("\nDistribusi kelas sebelum SMOTE:")
print(y.value_counts())

# Setelah SMOTE
print("\nDistribusi kelas setelah SMOTE:")
print(y_resampled.value_counts())

# Menampilkan jumlah data setelah SMOTE
print(f"\nJumlah data setelah SMOTE: {X_resampled.shape[0]}")

"""## Splitting"""

# Membagi data menjadi training set dan test set (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Menampilkan ukuran data setelah pembagian
print(f"Jumlah data untuk training: {X_train.shape[0]}")
print(f"Jumlah data untuk testing: {X_test.shape[0]}")

"""Modelling"""

# Fungsi evaluasi model untuk MAE, MSE, RMSE, dan R2
def evaluate(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = mse ** 0.5
    r2 = r2_score(y_true, y_pred)
    return mae, mse, rmse, r2

# Fungsi untuk cross-validation
def cross_val(model):
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')  # Negative MSE for cross_val_score
    return cv_scores.mean()

# --- Model 1: Linear Regression ---
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)

# Prediksi untuk test set dan train set
test_pred_lin_reg = lin_reg.predict(X_test)
train_pred_lin_reg = lin_reg.predict(X_train)

# Evaluasi Model Linear Regression untuk test set
mae_test_lr, mse_test_lr, rmse_test_lr, r2_test_lr = evaluate(y_test, test_pred_lin_reg)

# Evaluasi Model Linear Regression untuk train set
mae_train_lr, mse_train_lr, rmse_train_lr, r2_train_lr = evaluate(y_train, train_pred_lin_reg)

# Menampilkan hasil evaluasi untuk Linear Regression
print("Evaluasi Model 1: Linear Regression")
print(f"\nTest set evaluation (Linear Regression):")
print(f"MAE: {mae_test_lr}, MSE: {mse_test_lr}, RMSE: {rmse_test_lr}, R2: {r2_test_lr}")
print(f"\nTrain set evaluation (Linear Regression):")
print(f"MAE: {mae_train_lr}, MSE: {mse_train_lr}, RMSE: {rmse_train_lr}, R2: {r2_train_lr}")

# --- Model 2: Random Forest Regressor ---
rf = RandomForestRegressor(random_state=42)
rf.fit(X_train, y_train)

# Prediksi untuk test set dan train set
test_pred_rf = rf.predict(X_test)
train_pred_rf = rf.predict(X_train)

# Evaluasi Model Random Forest Regressor untuk test set
mae_test_rf, mse_test_rf, rmse_test_rf, r2_test_rf = evaluate(y_test, test_pred_rf)

# Evaluasi Model Random Forest Regressor untuk train set
mae_train_rf, mse_train_rf, rmse_train_rf, r2_train_rf = evaluate(y_train, train_pred_rf)

# Menampilkan hasil evaluasi untuk Random Forest Regressor
print("\nEvaluasi Model 2: Random Forest Regressor")
print(f"\nTest set evaluation (Random Forest):")
print(f"MAE: {mae_test_rf}, MSE: {mse_test_rf}, RMSE: {rmse_test_rf}, R2: {r2_test_rf}")
print(f"\nTrain set evaluation (Random Forest):")
print(f"MAE: {mae_train_rf}, MSE: {mse_train_rf}, RMSE: {rmse_train_rf}, R2: {r2_train_rf}")

# --- Model 3: Gradient Boosting Regressor ---
gb = GradientBoostingRegressor(random_state=42)
gb.fit(X_train, y_train)

# Prediksi untuk test set dan train set
test_pred_gb = gb.predict(X_test)
train_pred_gb = gb.predict(X_train)

# Evaluasi Model Gradient Boosting Regressor untuk test set
mae_test_gb, mse_test_gb, rmse_test_gb, r2_test_gb = evaluate(y_test, test_pred_gb)

# Evaluasi Model Gradient Boosting Regressor untuk train set
mae_train_gb, mse_train_gb, rmse_train_gb, r2_train_gb = evaluate(y_train, train_pred_gb)

# Menampilkan hasil evaluasi untuk Gradient Boosting Regressor
print("\nEvaluasi Model 3: Gradient Boosting Regressor")
print(f"\nTest set evaluation (Gradient Boosting):")
print(f"MAE: {mae_test_gb}, MSE: {mse_test_gb}, RMSE: {rmse_test_gb}, R2: {r2_test_gb}")
print(f"\nTrain set evaluation (Gradient Boosting):")
print(f"MAE: {mae_train_gb}, MSE: {mse_train_gb}, RMSE: {rmse_train_gb}, R2: {r2_train_gb}")

# Results
results_df = pd.DataFrame(data=[
    ["Linear Regression", mae_test_lr, mse_test_lr, rmse_test_lr, r2_test_lr, cross_val(lin_reg)],
    ["Random Forest Regressor", mae_test_rf, mse_test_rf, rmse_test_rf, r2_test_rf, cross_val(rf)],
    ["Gradient Boosting Regressor", mae_test_gb, mse_test_gb, rmse_test_gb, r2_test_gb, cross_val(gb)]
], columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', "Cross Validation"])

print("\nEvaluation Results:")
print(results_df)

"""## Hyperparameter Tuning"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy.stats import randint
import numpy as np

# Menyiapkan model Random Forest
rf = RandomForestRegressor(random_state=42)

# Menyusun distribusi parameter yang lebih ringan untuk Random Forest
param_dist_rf = {
    'n_estimators': [50, 100],  # Jumlah pohon lebih sedikit
    'max_depth': [None, 10],     # Kedalaman maksimum pohon
    'min_samples_split': [2, 5],  # Jumlah minimum sampel untuk membagi node
    'min_samples_leaf': [1, 2]   # Jumlah minimum sampel untuk daun
}

# Menerapkan RandomizedSearchCV untuk mencari parameter terbaik
random_search_rf = RandomizedSearchCV(estimator=rf, param_distributions=param_dist_rf,
                                      n_iter=5, cv=2, n_jobs=-1, random_state=42, scoring='neg_mean_squared_error')

# Menggunakan subset data untuk mempercepat proses
random_search_rf.fit(X_train[:1000], y_train[:1000])  # Gunakan hanya 1000 baris pertama

# Menampilkan hasil terbaik dari RandomizedSearchCV
print("Best parameters for Random Forest Regressor:", random_search_rf.best_params_)

# Model terbaik berdasarkan RandomizedSearchCV
best_rf = random_search_rf.best_estimator_

# Evaluasi hasil model terbaik pada data test
mae_test_rf = mean_absolute_error(y_test, best_rf.predict(X_test))
mse_test_rf = mean_squared_error(y_test, best_rf.predict(X_test))
rmse_test_rf = np.sqrt(mse_test_rf)  # Menghitung RMSE secara manual
r2_test_rf = r2_score(y_test, best_rf.predict(X_test))

# Hasil evaluasi model
results_df = pd.DataFrame(data=[
    ["Random Forest Regressor", mae_test_rf, mse_test_rf, rmse_test_rf, r2_test_rf]
], columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square'])

print("\nEvaluation Results after Hyperparameter Tuning:")
print(results_df)

# Feature importance untuk Random Forest
importances = best_rf.feature_importances_
sorted_indices = importances.argsort()

plt.figure(figsize=(10, 6))
plt.barh(range(len(sorted_indices)), importances[sorted_indices], align='center')
plt.yticks(range(len(sorted_indices)), [df.drop('loan_status', axis=1).columns[i] for i in sorted_indices]) # Use df.columns to get column names
plt.title('Feature Importance (Random Forest)')
plt.show()